from fastapi import FastAPI, HTTPException, Query, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
import uvicorn
from contextlib import asynccontextmanager
import logging
from datetime import datetime
from typing import List, Optional
from sqlalchemy import text
from sqlalchemy.exc import SQLAlchemyError

# å¯¼å…¥ä¸¤ä¸ªAPIæ¨¡å—
from maritime_api import maritime_app
from business_api import business_app
from cosco_api import cosco_router

# å¯¼å…¥æ–°å¢çš„èˆ¹èˆ¶ç›¸å…³è·¯ç”±
from vessel_purchase_risk import purchase_router
from vessel_second_hand_disposal_risk import second_hand_router as disposal_router
from vessle_charter_in_risk import charter_in_router
from vessel_second_hand_disassemble_risk import disassemble_router
# å¯¼å…¥ä»“å‚¨ç å¤´åˆè§„ç­›æŸ¥è·¯ç”±
from warehousing_wharf_risk import warehousing_router

# é…ç½®æ—¥å¿—
import logging
import sys
import json

def safe_json_parse(json_str, default_value=None):
    """å®‰å…¨è§£æJSONå­—ç¬¦ä¸²ï¼Œå¤„ç†å¯èƒ½çš„æ ¼å¼é”™è¯¯"""
    if not json_str or json_str in ("null", "None"):
        return default_value
    try:
        return json.loads(json_str)
    except (json.JSONDecodeError, TypeError):
        return default_value

# é…ç½®æ ¹æ—¥å¿—è®°å½•å™¨
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),  # ç¡®ä¿è¾“å‡ºåˆ°stdout
    ],
    force=True  # å¼ºåˆ¶é‡æ–°é…ç½®æ—¥å¿—
)

logger = logging.getLogger(__name__)

# ç¡®ä¿æ‰€æœ‰æ—¥å¿—éƒ½ç«‹å³åˆ·æ–°
class FlushingStreamHandler(logging.StreamHandler):
    def emit(self, record):
        super().emit(record)
        self.flush()

# æ·»åŠ ä¸€ä¸ªè‡ªå®šä¹‰çš„æ—¥å¿—å¤„ç†å™¨
flushing_handler = FlushingStreamHandler(sys.stdout)
flushing_handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
flushing_handler.setFormatter(formatter)

# ä¸ºæ ¹æ—¥å¿—è®°å½•å™¨æ·»åŠ å¤„ç†å™¨
root_logger = logging.getLogger()
root_logger.addHandler(flushing_handler)

# å…¨å±€å¼‚å¸¸å¤„ç†å™¨
import sys
import traceback
from fastapi.responses import JSONResponse

def global_exception_handler(request, exc):
    """å…¨å±€å¼‚å¸¸å¤„ç†å™¨ï¼Œé˜²æ­¢æœåŠ¡å´©æºƒ"""
    logger.error(f"æœªå¤„ç†çš„å¼‚å¸¸: {exc}")
    logger.error(f"å¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
    return JSONResponse(
        status_code=500,
        content={
            "error": "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯",
            "message": "æœåŠ¡é‡åˆ°å¼‚å¸¸ä½†ä»åœ¨è¿è¡Œ",
            "timestamp": datetime.now().isoformat()
        }
    )

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    import os
    # å¯åŠ¨æ—¶æ‰§è¡Œ - ç®€åŒ–æ—¥å¿—è¾“å‡º
    try:
        # åŠ è½½COSCO APIï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼Œé¿å…é‡å¤åŠ è½½ï¼‰
        from cosco_api import cosco_router
        app.include_router(cosco_router, prefix="/cosco", tags=["COSCOæ¨¡å‹ç®—æ³•API"])
    except Exception as e:
        logger.warning(f"âš ï¸ COSCOæ¨¡å‹ç®—æ³•APIåŠ è½½å¤±è´¥: {e}")
    
    try:
        # åŠ è½½æµ·äº‹æ•°æ®ç»¼åˆAPI
        from maritime_api import maritime_app
        app.mount("/maritime", maritime_app)
    except Exception as e:
        logger.warning(f"âš ï¸ æµ·äº‹æ•°æ®ç»¼åˆAPIåŠ è½½å¤±è´¥: {e}")
    
    try:
        # åŠ è½½ä¸šåŠ¡é€»è¾‘API
        from business_api import VesselOut, VesselRiskResponse, QueryResponse
    except Exception as e:
        logger.warning(f"âš ï¸ ä¸šåŠ¡é€»è¾‘APIåŠ è½½å¤±è´¥: {e}")
    
    try:
        # åŠ è½½èˆªæ¬¡æœåŠ¡API
        from external_api import external_router
        app.include_router(external_router, prefix="/external", tags=["èˆªæ¬¡æœåŠ¡API"])
    except Exception as e:
        logger.warning(f"âš ï¸ èˆªæ¬¡æœåŠ¡APIåŠ è½½å¤±è´¥: {e}")
    
    try:
        # åŠ è½½STSé£é™©ç­›æŸ¥API
        from sts_bunkering_risk import sts_router
        app.include_router(sts_router, prefix="/sts", tags=["STSé£é™©ç­›æŸ¥API"])
    except Exception as e:
        logger.warning(f"âš ï¸ STSé£é™©ç­›æŸ¥APIåŠ è½½å¤±è´¥: {e}")
    
    try:
        # åŠ è½½èˆªæ¬¡åˆè§„çŠ¶æ€å®¡æ‰¹APIï¼ˆç»Ÿä¸€ä¸º /external å‰ç¼€ï¼Œä¸ä¸»ç‰ˆæœ¬ä¸€è‡´ï¼‰
        from external_voyage_approval_api import external_approval_router
        app.include_router(external_approval_router, prefix="/external", tags=["èˆªæ¬¡åˆè§„çŠ¶æ€å®¡æ‰¹API"])
    except Exception as e:
        logger.warning(f"âš ï¸ èˆªæ¬¡åˆè§„çŠ¶æ€å®¡æ‰¹APIåŠ è½½å¤±è´¥: {e}")
    
    # åŠ è½½èˆ¹èˆ¶ç›¸å…³è·¯ç”±
    try:
        app.include_router(purchase_router, tags=["èˆ¹èˆ¶ä¹°å…¥åˆè§„çŠ¶æ€ç­›æŸ¥API"])
        logger.info("âœ… èˆ¹èˆ¶ä¹°å…¥åˆè§„çŠ¶æ€ç­›æŸ¥APIåŠ è½½æˆåŠŸ")
    except Exception as e:
        logger.warning(f"âš ï¸ èˆ¹èˆ¶ä¹°å…¥åˆè§„çŠ¶æ€ç­›æŸ¥APIåŠ è½½å¤±è´¥: {e}")
    
    try:
        app.include_router(disposal_router, tags=["äºŒæ‰‹èˆ¹å‡ºå”®åˆè§„çŠ¶æ€ç­›æŸ¥API"])
        logger.info("âœ… äºŒæ‰‹èˆ¹å‡ºå”®åˆè§„çŠ¶æ€ç­›æŸ¥APIåŠ è½½æˆåŠŸ")
    except Exception as e:
        logger.warning(f"âš ï¸ äºŒæ‰‹èˆ¹å‡ºå”®åˆè§„çŠ¶æ€ç­›æŸ¥APIåŠ è½½å¤±è´¥: {e}")
    
    try:
        app.include_router(charter_in_router, tags=["èˆ¹èˆ¶ç§Ÿå…¥åˆè§„çŠ¶æ€ç­›æŸ¥API"])
        logger.info("âœ… èˆ¹èˆ¶ç§Ÿå…¥åˆè§„çŠ¶æ€ç­›æŸ¥APIåŠ è½½æˆåŠŸ")
    except Exception as e:
        logger.warning(f"âš ï¸ èˆ¹èˆ¶ç§Ÿå…¥åˆè§„çŠ¶æ€ç­›æŸ¥APIåŠ è½½å¤±è´¥: {e}")
    
    try:
        app.include_router(disassemble_router, tags=["äºŒæ‰‹èˆ¹æ‹†è§£åˆè§„çŠ¶æ€ç­›æŸ¥API"])
        logger.info("âœ… äºŒæ‰‹èˆ¹æ‹†è§£åˆè§„çŠ¶æ€ç­›æŸ¥APIåŠ è½½æˆåŠŸ")
    except Exception as e:
        logger.warning(f"âš ï¸ äºŒæ‰‹èˆ¹æ‹†è§£åˆè§„çŠ¶æ€ç­›æŸ¥APIåŠ è½½å¤±è´¥: {e}")
        
    try:
        app.include_router(warehousing_router, tags=["ä»“å‚¨ç å¤´åˆè§„çŠ¶æ€ç­›æŸ¥API"])
        logger.info("âœ… ä»“å‚¨ç å¤´åˆè§„çŠ¶æ€ç­›æŸ¥APIåŠ è½½æˆåŠŸ")
    except Exception as e:
        logger.warning(f"âš ï¸ ä»“å‚¨ç å¤´åˆè§„çŠ¶æ€ç­›æŸ¥APIåŠ è½½å¤±è´¥: {e}")
    
    # åªåœ¨ä¸»è¿›ç¨‹æ˜¾ç¤ºå¯åŠ¨å®Œæˆä¿¡æ¯
    if os.getenv("UVICORN_WORKER_ID") is None:
        logger.info("ğŸ“Š æ‰€æœ‰æ¨¡å—åŠ è½½å®Œæˆï¼")
    
    yield
    
    # å…³é—­æ—¶æ‰§è¡Œ
    if os.getenv("UVICORN_WORKER_ID") is None:
        logger.info("ğŸ›‘ æœåŠ¡æ­£åœ¨å…³é—­...")

# åˆ›å»ºä¸»åº”ç”¨
main_app = FastAPI(
    title="èˆ¹èˆ¶ä¿¡æ¯ç»¼åˆAPIæœåŠ¡",
    version="2.0.0",
    description="""
    # èˆ¹èˆ¶ä¿¡æ¯ç»¼åˆAPIæœåŠ¡
    
    è¿™æ˜¯ä¸€ä¸ªæ•´åˆäº†å¤šä¸ªèˆ¹èˆ¶ç›¸å…³APIçš„ç»¼åˆæœåŠ¡ï¼ŒåŒ…æ‹¬ï¼š
    
    ## ğŸš¢ æµ·äº‹æ•°æ®ç»¼åˆAPI (maritime_api)
    - **Lloyd'såˆè§„æ•°æ®**: è·å–èˆ¹èˆ¶åˆè§„ä¿¡æ¯
    - **Lloyd'såˆ¶è£æ•°æ®**: æŸ¥è¯¢èˆ¹èˆ¶åˆ¶è£çŠ¶æ€
    - **UANIæ•°æ®**: æ£€æŸ¥èˆ¹èˆ¶æ˜¯å¦åœ¨UANIæ¸…å•ä¸­
    - **Kpleræ•°æ®åˆ†æ**: æ‰§è¡Œèˆ¹èˆ¶é£é™©åˆ†æ
    - **èˆªæ¬¡é£é™©åˆ†æ**: åˆ†æèˆ¹èˆ¶èˆªæ¬¡é£é™©
    - **èˆ¹èˆ¶çŠ¶æ€ç»¼åˆè¯„ä¼°**: ç»¼åˆåˆ¤æ–­èˆ¹èˆ¶é£é™©ç­‰çº§
    
    ## ğŸ¢ ä¸šåŠ¡é€»è¾‘API (business_api)
    - **ä¼ä¸šåˆ¶è£é£é™©æŸ¥è¯¢**: æ ¹æ®å…¬å¸åç§°æŸ¥è¯¢åˆ¶è£é£é™©
    - **èˆ¹èˆ¶é£é™©ä¿¡æ¯æŸ¥è¯¢**: æ ¹æ®IMOç¼–å·æŸ¥è¯¢èˆ¹èˆ¶é£é™©
    - **èˆ¹èˆ¶é£é™©æ•°æ®ä¿å­˜**: ä¿å­˜èˆ¹èˆ¶é£é™©åˆ†æç»“æœ
    
    ## ğŸ­ COSCOæ¨¡å‹ç®—æ³•API (cosco_api)
    - **å…¬å¸åç§°åŒ¹é…**: å®æ—¶åŒ¹é…å…¬å¸åç§°ï¼Œæ”¯æŒç²¾ç¡®åŒ¹é…å’Œæ¨¡ç³ŠåŒ¹é…
    - **æ™ºèƒ½ç´¢å¼•**: åŸºäºWhooshçš„å¿«é€Ÿæœç´¢ç´¢å¼•
    - **å¤šè¯­è¨€æ”¯æŒ**: æ”¯æŒä¸­æ–‡ã€è‹±æ–‡ç­‰å¤šç§è¯­è¨€çš„å…¬å¸åç§°å¤„ç†
    - **ç¼“å­˜æœºåˆ¶**: æ™ºèƒ½ç¼“å­˜å’Œè‡ªåŠ¨åˆ·æ–°æœºåˆ¶

    ## âš“ èˆ¹èˆ¶äº¤æ˜“ç›¸å…³API
    - **èˆ¹èˆ¶ä¹°å…¥åˆè§„ç­›æŸ¥**: è¯„ä¼°èˆ¹èˆ¶ä¹°å…¥çš„åˆè§„é£é™©
    - **äºŒæ‰‹èˆ¹å‡ºå”®åˆè§„ç­›æŸ¥**: è¯„ä¼°äºŒæ‰‹èˆ¹å‡ºå”®çš„åˆè§„é£é™©
    - **èˆ¹èˆ¶ç§Ÿå…¥åˆè§„ç­›æŸ¥**: è¯„ä¼°èˆ¹èˆ¶ç§Ÿå…¥çš„åˆè§„é£é™©
    - **äºŒæ‰‹èˆ¹æ‹†è§£åˆè§„ç­›æŸ¥**: è¯„ä¼°äºŒæ‰‹èˆ¹æ‹†è§£çš„åˆè§„é£é™©
    
    ## ğŸ”§ æŠ€æœ¯ç‰¹æ€§
    - æ”¯æŒCORSè·¨åŸŸè¯·æ±‚
    - Gzipå‹ç¼©å“åº”
    - å®Œæ•´çš„é”™è¯¯å¤„ç†
    - è¯¦ç»†çš„APIæ–‡æ¡£
    - å¥åº·æ£€æŸ¥æ¥å£
    
    ## ğŸ“š APIæ–‡æ¡£
    - è®¿é—® `/docs` æŸ¥çœ‹Swagger UIæ–‡æ¡£
    - è®¿é—® `/redoc` æŸ¥çœ‹ReDocæ–‡æ¡£
    """,
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan
)

# å¯¼å…¥æ€§èƒ½ç›‘æ§ä¸­é—´ä»¶
from performance_monitor import PerformanceMonitorMiddleware, get_performance_stats

# æ·»åŠ ä¸­é—´ä»¶
main_app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥é™åˆ¶å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

main_app.add_middleware(GZipMiddleware, minimum_size=1000)

# æ·»åŠ æ€§èƒ½ç›‘æ§ä¸­é—´ä»¶
main_app.add_middleware(PerformanceMonitorMiddleware)

# æ³¨å†Œå…¨å±€å¼‚å¸¸å¤„ç†å™¨
main_app.add_exception_handler(Exception, global_exception_handler)

# æŒ‚è½½æµ·äº‹APIå­åº”ç”¨
main_app.mount("/maritime", maritime_app, name="æµ·äº‹æ•°æ®ç»¼åˆAPI")

# æŒ‚è½½COSCOæ¨¡å‹ç®—æ³•APIè·¯ç”±
main_app.include_router(cosco_router)

# æŒ‚è½½èˆªæ¬¡æœåŠ¡APIè·¯ç”±
from external_api import external_router
main_app.include_router(external_router)

# æŒ‚è½½STSé£é™©ç­›æŸ¥APIè·¯ç”±
from sts_bunkering_risk import sts_router
main_app.include_router(sts_router)

# æŒ‚è½½èˆªæ¬¡åˆè§„çŠ¶æ€å®¡æ‰¹APIè·¯ç”±ï¼ˆç»Ÿä¸€ä½¿ç”¨ /external å‰ç¼€ï¼‰
from external_voyage_approval_api import external_approval_router
main_app.include_router(external_approval_router, prefix="/external")

# æŒ‚è½½èˆ¹èˆ¶ç›¸å…³è·¯ç”±
main_app.include_router(purchase_router)
main_app.include_router(disposal_router)
main_app.include_router(charter_in_router)
main_app.include_router(disassemble_router)
from warehousing_wharf_risk import warehousing_router
main_app.include_router(warehousing_router)

# å¯¼å…¥ä¸šåŠ¡APIçš„è·¯ç”±å’Œæ¨¡å‹ï¼Œç›´æ¥åœ¨ä¸»åº”ç”¨ä¸­å®šä¹‰
from business_api import (
    VesselOut, VesselRiskResponse, QueryResponse,
    get_db, fetch_one, BASE_SQL, DB_CONFIG
)
import psycopg2
import json
from fastapi import Request
from sqlalchemy.orm import Session

# ä¸»åº”ç”¨è·¯ç”±
@main_app.get("/")
async def root():
    """ä¸»åº”ç”¨æ ¹è·¯å¾„"""
    return {
        "message": "èˆ¹èˆ¶ä¿¡æ¯ç»¼åˆAPIæœåŠ¡",
        "version": "2.0.0",
        "description": "æ•´åˆäº†æµ·äº‹æ•°æ®ç»¼åˆAPIå’Œä¸šåŠ¡é€»è¾‘APIçš„ç»¼åˆæœåŠ¡",
        "timestamp": datetime.now().isoformat(),
        "modules": {
            "maritime_api": {
                "name": "æµ·äº‹æ•°æ®ç»¼åˆAPI",
                "base_path": "/maritime",
                "description": "æ•´åˆLloyd'sã€Kplerå’ŒUANIæ•°æ®çš„æµ·äº‹é£é™©åˆ†ææœåŠ¡",
                "endpoints": [
                    "/maritime/api/execute_full_analysis",
                    "/maritime/api/lloyds_compliance",
                    "/maritime/api/lloyds_sanctions",
                    "/maritime/api/uani_check",
                    "/maritime/api/kpler_analysis",
                    "/maritime/api/voyage_risks",
                    "/maritime/api/vessel_status",
                    "/maritime/api/save_results"
                ]
            },
            "external_api": {
                "name": "èˆªæ¬¡æœåŠ¡API",
                "base_path": "/external",
                "description": "èˆªæ¬¡é£é™©ç­›æŸ¥å’Œèˆ¹èˆ¶åŸºç¡€ä¿¡æ¯æŸ¥è¯¢æœåŠ¡",
                "endpoints": [
                    "/external/voyage_risk (POST)",
                    "/external/vessel_basic (POST)"
                ]
            },
            "sts_api": {
                "name": "STSé£é™©ç­›æŸ¥API",
                "base_path": "/sts",
                "description": "STSï¼ˆèˆ¹å¯¹èˆ¹ï¼‰é£é™©ç­›æŸ¥æœåŠ¡ï¼ŒåŒ…å«èˆ¹èˆ¶ç›¸å…³æ–¹åˆ¶è£ã€AISä¿¡å·åˆ†æç­‰",
                "endpoints": [
                    "/sts/risk_screen (POST)"
                ]
            },
            "external_approval_api": {
                "name": "èˆªæ¬¡åˆè§„çŠ¶æ€å®¡æ‰¹API",
                "base_path": "/external",
                "description": "èˆªæ¬¡åˆè§„çŠ¶æ€å®¡æ‰¹ä¿¡æ¯è¿”å›æœåŠ¡ï¼Œæä¾›èˆªæ¬¡é£é™©ç­›æŸ¥ç»“æœ",
                "endpoints": [
                    "/external/voyage_approval (POST)"
                ]
            },
            "cosco_api": {
                "name": "COSCOæ¨¡å‹ç®—æ³•API",
                "base_path": "/cosco",
                "description": "å…¬å¸åç§°å®æ—¶åŒ¹é…æœåŠ¡",
                "endpoints": [
                    "/cosco/match",
                    "/cosco/health",
                    "/cosco/info",
                    "/cosco/refresh-data"
                ]
            },
            "purchase_api": {
                "name": "èˆ¹èˆ¶ä¹°å…¥åˆè§„çŠ¶æ€ç­›æŸ¥API",
                "base_path": "/purchase",
                "description": "èˆ¹èˆ¶ä¹°å…¥åˆè§„çŠ¶æ€ç­›æŸ¥æœåŠ¡",
                "endpoints": [
                    "/purchase/vessel_purchase_risk (POST)"
                ]
            },
            "second_hand_disposal_api": {
                "name": "äºŒæ‰‹èˆ¹å‡ºå”®åˆè§„çŠ¶æ€ç­›æŸ¥API",
                "base_path": "/second_hand",
                "description": "äºŒæ‰‹èˆ¹å‡ºå”®åˆè§„çŠ¶æ€ç­›æŸ¥æœåŠ¡",
                "endpoints": [
                    "/second_hand/vessel_disposal_risk (POST)"
                ]
            },
            "charter_in_api": {
                "name": "èˆ¹èˆ¶ç§Ÿå…¥åˆè§„çŠ¶æ€ç­›æŸ¥API",
                "base_path": "/charter_in",
                "description": "èˆ¹èˆ¶ç§Ÿå…¥åˆè§„çŠ¶æ€ç­›æŸ¥æœåŠ¡",
                "endpoints": [
                    "/charter_in/vessel_charter_risk (POST)"
                ]
            },
            "second_hand_disassemble_api": {
                "name": "äºŒæ‰‹èˆ¹æ‹†è§£åˆè§„çŠ¶æ€ç­›æŸ¥API",
                "base_path": "/second_hand",
                "description": "äºŒæ‰‹èˆ¹æ‹†è§£åˆè§„çŠ¶æ€ç­›æŸ¥æœåŠ¡",
                "endpoints": [
                    "/second_hand/vessel_disassemble_risk (POST)"
                ]
            },
            "warehousing_api": {
                "name": "ä»“å‚¨ç å¤´åˆè§„çŠ¶æ€ç­›æŸ¥API",
                "base_path": "/warehousing",
                "description": "ä»“å‚¨ç å¤´åˆè§„çŠ¶æ€ç­›æŸ¥æœåŠ¡",
                "endpoints": [
                    "/warehousing/wharf_risk (POST)"
                ]
            },
            "main_app": {
                "name": "ä¸»åº”ç”¨API",
                "base_path": "/",
                "description": "èˆ¹èˆ¶ä¿¡æ¯æŸ¥è¯¢å’Œé£é™©æ•°æ®ç®¡ç†æœåŠ¡",
                "endpoints": [
                    "/query_by_name (æ”¯æŒå†å²è®°å½•)",
                    "/search-entities/ (åŸºç¡€æŸ¥è¯¢)",
                    "/search-vessel/",
                    "/api/save_vessel_data",
                    "/api/get_vessel_all_data",
                    "/api/sts_data",
                    "/api/vessel_list"
                ]
            }
        },
        "documentation": {
            "swagger_ui": "/docs",
            "redoc": "/redoc",
            "health_check": "/health"
        }
    }

@main_app.get("/performance")
async def get_performance_metrics():
    """è·å–æ€§èƒ½ç›‘æ§æŒ‡æ ‡"""
    return get_performance_stats()

@main_app.get("/health")
async def health_check():
    """ä¸»åº”ç”¨å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "service": "èˆ¹èˆ¶ä¿¡æ¯ç»¼åˆAPIæœåŠ¡",
        "version": "2.0.0",
        "timestamp": datetime.now().isoformat(),
        "modules": {
            "maritime_api": "active",
            "external_api": "active",
            "sts_api": "active",
            "external_approval_api": "active",
            "cosco_api": "active",
            "business_api": "active",
            "purchase_api": "active",
            "second_hand_disposal_api": "active",
            "charter_in_api": "active",
            "second_hand_disassemble_api": "active"
        }
    }

# æ·»åŠ ç¼ºå¤±çš„APIç«¯ç‚¹ï¼Œä¿æŒå‘åå…¼å®¹æ€§
@main_app.get("/api/get_vessel_all_data")
async def get_vessel_all_data_main(
    vessel_imo: str = Query(..., description="èˆ¹èˆ¶IMOå·"),
    start_date: str = Query(None, description="å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤ä¸º1å¹´å‰çš„ä»Šå¤©"),
    end_date: str = Query(None, description="ç»“æŸæ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤ä¸ºå½“å‰æ—¥æœŸ")
):
    """è·å–èˆ¹èˆ¶æ‰€æœ‰ç›¸å…³æ•°æ®ï¼ˆä¸€æ¬¡æ€§è°ƒç”¨æ‰€æœ‰æ¥å£ï¼‰- ä¸»åº”ç”¨å…¼å®¹ç«¯ç‚¹"""
    try:
        # å¯¼å…¥maritime_apiä¸­çš„å‡½æ•°
        from maritime_api import get_vessel_all_data
        return await get_vessel_all_data(vessel_imo, start_date, end_date)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–èˆ¹èˆ¶æ•°æ®å¤±è´¥: {str(e)}")

@main_app.get("/api/sts_data")
async def get_sts_data_main(
    vessel_imo: str = Query(..., description="èˆ¹èˆ¶IMOå·")
):
    """è·å–èˆ¹èˆ¶STSï¼ˆèˆ¹å¯¹èˆ¹è½¬è¿ï¼‰æ•°æ® - ä¸»åº”ç”¨å…¼å®¹ç«¯ç‚¹"""
    try:
        # å¯¼å…¥maritime_apiä¸­çš„å‡½æ•°
        from maritime_api import get_sts_data
        return await get_sts_data(vessel_imo)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–STSæ•°æ®å¤±è´¥: {str(e)}")

@main_app.get("/api/vessel_list")
async def get_vessel_list_main(
    vessel_imo: str = Query(..., description="èˆ¹èˆ¶IMOå·")
):
    """è·å–èˆ¹èˆ¶åŸºç¡€ä¿¡æ¯ - ä¸»åº”ç”¨å…¼å®¹ç«¯ç‚¹"""
    try:
        # å¯¼å…¥maritime_apiä¸­çš„å‡½æ•°
        from maritime_api import get_vessel_basic_info
        return await get_vessel_basic_info(vessel_imo)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–èˆ¹èˆ¶åŸºç¡€ä¿¡æ¯å¤±è´¥: {str(e)}")

@main_app.get("/info")
async def get_service_info():
    """è·å–æœåŠ¡è¯¦ç»†ä¿¡æ¯"""
    return {
        "service_name": "èˆ¹èˆ¶ä¿¡æ¯ç»¼åˆAPIæœåŠ¡",
        "version": "2.0.0",
        "startup_time": datetime.now().isoformat(),
        "total_endpoints": 19,  # åŸæœ‰15ä¸ª + æ–°å¢4ä¸ªèˆ¹èˆ¶ç›¸å…³API
        "features": [
            "Lloyd's APIé›†æˆ",
            "Kpler APIé›†æˆ", 
            "UANIæ•°æ®çˆ¬å–",
            "èˆ¹èˆ¶é£é™©åˆ†æ",
            "ä¼ä¸šåˆ¶è£é£é™©æŸ¥è¯¢ï¼ˆæ”¯æŒå†å²è®°å½•ï¼‰",
            "èˆ¹èˆ¶é£é™©æ•°æ®ç®¡ç†",
            "èˆªæ¬¡é£é™©åˆ†æ",
            "ç»¼åˆé£é™©è¯„ä¼°",
            "èˆ¹èˆ¶ä¹°å…¥åˆè§„ç­›æŸ¥",
            "äºŒæ‰‹èˆ¹å‡ºå”®åˆè§„ç­›æŸ¥",
            "èˆ¹èˆ¶ç§Ÿå…¥åˆè§„ç­›æŸ¥",
            "äºŒæ‰‹èˆ¹æ‹†è§£åˆè§„ç­›æŸ¥"
        ],
        "api_groups": [
            {
                "name": "æµ·äº‹æ•°æ®ç»¼åˆAPI",
                "base_path": "/maritime",
                "endpoint_count": 8,
                "description": "æµ·äº‹é£é™©åˆ†æç›¸å…³æ¥å£"
            },
            {
                "name": "ä¸šåŠ¡é€»è¾‘API", 
                "base_path": "/business",
                "endpoint_count": 7,
                "description": "ä¸šåŠ¡æŸ¥è¯¢å’Œæ•°æ®ç®¡ç†æ¥å£ï¼ˆåŒ…å«å…¼å®¹ç«¯ç‚¹ï¼‰"
            },
            {
                "name": "èˆ¹èˆ¶äº¤æ˜“ç›¸å…³API",
                "base_path": "/",
                "endpoint_count": 4,
                "description": "èˆ¹èˆ¶ä¹°å…¥ã€å‡ºå”®ã€ç§Ÿå…¥å’Œæ‹†è§£çš„åˆè§„ç­›æŸ¥æ¥å£"
            }
        ]
    }

# ä¸šåŠ¡APIè·¯ç”± - ç›´æ¥åœ¨ä¸»åº”ç”¨ä¸­å®šä¹‰
@main_app.get("/query_by_name", response_model=QueryResponse)
async def query_by_name(
    entity_id: str = Query(..., min_length=1, description="ä¼ä¸šidæŸ¥è¯¢"),
    page: int = Query(1, ge=1, description="é¡µç "),
    page_size: int = Query(10, ge=1, le=100, description="æ¯é¡µæ¡æ•°"),
    search_time: Optional[str] = Query(None, description="æŸ¥è¯¢æ—¶é—´ï¼Œæ ¼å¼ä¸ºYYYY-MM-DD HH:mm:ss"),
    user_id: Optional[str] = Query(None, description="ç”¨æˆ·ID"),
    user_name: Optional[str] = Query(None, description="ç”¨æˆ·åç§°"),
    depart_id: Optional[str] = Query(None, description="éƒ¨é—¨ID"),
    depart_name: Optional[str] = Query(None, description="éƒ¨é—¨åç§°")
):
    """æ ¹æ®ä¼ä¸šIDæŸ¥è¯¢åˆ¶è£é£é™©ç»“æœï¼ˆæ”¯æŒåˆ†é¡µï¼‰å¹¶è®°å½•æŸ¥è¯¢å†å²"""
    conn = None
    try:
        conn = psycopg2.connect(** DB_CONFIG)
        with conn.cursor() as cursor:
            offset = (page - 1) * page_size

            sql = """
            SELECT 
                entity_id, entity_dt, activestatus, ENTITYNAME1, ENTITYNAME4,
                country_nm1, country_nm2, DATEVALUE1, sanctions_lev,
                sanctions_list, mid_sanctions_list, no_sanctions_list, unknown_risk_list, other_list
            FROM lng.sanctions_risk_result
            WHERE entity_id = %s
            LIMIT %s OFFSET %s
            """
            cursor.execute(sql, [entity_id, page_size, offset])
            results = cursor.fetchall()

            count_sql = "SELECT COUNT(*) AS total FROM lng.sanctions_risk_result WHERE entity_id = %s"
            cursor.execute(count_sql, [entity_id])
            total = cursor.fetchone()['total']

            if not results:
                return {
                    "total": 0,
                    "data": [],
                    "message": "æš‚æ— æ•°æ®"
                }

            processed_results = []
            for item in results:
                processed_item = {
                    "entity_id": str(item["entity_id"]),  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    "entity_dt": item.get("entity_dt"),
                    "activestatus": item.get("activestatus"),
                    "ENTITYNAME1": item.get("ENTITYNAME1"),
                    "ENTITYNAME4": item.get("ENTITYNAME4"),
                    "country_nm1": item.get("country_nm1"),
                    "country_nm2": item.get("country_nm2"),
                    "DATEVALUE1": item.get("DATEVALUE1"),
                    "sanctions_lev": item.get("sanctions_lev"),
                    "sanctions_list": safe_json_parse(item.get("sanctions_list"), []),
                    "mid_sanctions_list": safe_json_parse(item.get("mid_sanctions_list"), []),
                    "no_sanctions_list": safe_json_parse(item.get("no_sanctions_list"), []),
                    "unknown_risk_list": safe_json_parse(item.get("unknown_risk_list"), []),
                    "other_list": safe_json_parse(item.get("other_list"), [])
                }
                processed_results.append(processed_item)

            # è®°å½•æŸ¥è¯¢å†å²ï¼ˆå¦‚æœæä¾›äº†ç”¨æˆ·ä¿¡æ¯ï¼‰
            if user_id and user_name:
                try:
                    # è·å–ç¬¬ä¸€ä¸ªç»“æœç”¨äºå†å²è®°å½•
                    first_result = results[0] if results else None
                    if first_result:
                        actual_search_time = search_time if search_time else datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        
                        # æ„å»ºå†å²è®°å½•
                        his_record = {
                            "entity_id": str(first_result["entity_id"]),  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                            "risk_lev": first_result.get("sanctions_lev"),
                            "risk_type": "åˆ¶è£é£é™©æŸ¥è¯¢",  # å›ºå®šé£é™©ç±»å‹
                            "ENTITYNAME1": first_result.get("ENTITYNAME1"),
                            "ENTITYNAME4": first_result.get("ENTITYNAME4"),
                            "serch_time": actual_search_time,
                            "create_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                            "user_id": user_id,
                            "user_name": user_name,
                            "depart_id": depart_id,
                            "depart_name": depart_name,
                            "is_delete": "0",
                            "entity_dt": first_result.get("entity_dt"),
                            "activestatus": first_result.get("activestatus"),
                            "sanctions_lev": first_result.get("sanctions_lev"),
                            "country_nm1": first_result.get("country_nm1"),
                            "country_nm2": first_result.get("country_nm2"),
                            "DATEVALUE1": first_result.get("DATEVALUE1"),
                            "sanctions_list": json.dumps(first_result.get("sanctions_list"), ensure_ascii=False) if first_result.get("sanctions_list") else None,
                            "mid_sanctions_list": json.dumps(first_result.get("mid_sanctions_list"), ensure_ascii=False) if first_result.get("mid_sanctions_list") else None,
                            "no_sanctions_list": json.dumps(first_result.get("no_sanctions_list"), ensure_ascii=False) if first_result.get("no_sanctions_list") else None,
                            "unknown_risk_list": json.dumps(first_result.get("unknown_risk_list"), ensure_ascii=False) if first_result.get("unknown_risk_list") else None,
                            "other_list": json.dumps(first_result.get("other_list"), ensure_ascii=False) if first_result.get("other_list") else None
                        }
                        
                        # æ’å…¥å†å²è®°å½•
                        columns = []
                        placeholders = []
                        values = []
                        for k, v in his_record.items():
                            if v is not None:
                                columns.append(k)
                                placeholders.append("%s")
                                values.append(v)
                        
                        insert_sql = f"""
                        INSERT INTO lng.sanctions_risk_result_his 
                        ({', '.join(columns)})
                        VALUES ({', '.join(placeholders)})
                        """
                        cursor.execute(insert_sql, values)
                        conn.commit()
                        logger.info(f"æŸ¥è¯¢å†å²è®°å½•å·²ä¿å­˜: {entity_id}")
                        
                except Exception as e:
                    logger.error(f"ä¿å­˜æŸ¥è¯¢å†å²è®°å½•å¤±è´¥: {str(e)}")
                    # ä¸å½±å“ä¸»æŸ¥è¯¢ç»“æœï¼Œåªè®°å½•é”™è¯¯æ—¥å¿—

            return {"total": total, "data": processed_results}

    except psycopg2.Error as e:
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")
    except json.JSONDecodeError as e:
        raise HTTPException(status_code=500, detail=f"JSON parsing error: {str(e)}")
    finally:
        if conn:
            conn.close()

# @main_app.get("/search-entities/", response_model=List[VesselOut])
# def search_entities(
#     entity_id: Optional[str] = Query(None),
#     search_time: Optional[str] = Query(None, description="æŸ¥è¯¢æ—¶é—´ï¼Œæ ¼å¼ä¸ºYYYY-MM-DD HH:mm:ss"),
#     user_id: Optional[str] = Query(None),
#     user_name: Optional[str] = Query(None),
#     depart_id: Optional[str] = Query(None),
#     depart_name: Optional[str] = Query(None)
# ):
#     """å…¬å¸é£é™©æŸ¥è¯¢ï¼ˆå·²ç§»é™¤å†å²è®°å½•å­˜å‚¨åŠŸèƒ½ï¼Œè¯¥åŠŸèƒ½å·²è¿ç§»åˆ°query_by_nameæ¥å£ï¼‰"""
#     if not entity_id:
#         raise HTTPException(status_code=400, detail="entityname1å‚æ•°ä¸èƒ½ä¸ºç©º")
    
#     sql = f"""
#     {BASE_SQL}
#     WHERE entityname1 LIKE CONCAT('%', :entityname1, '%')
#     LIMIT 1
#     """
    
#     try:
#         logger.info(f"æ‰§è¡ŒæŸ¥è¯¢: {sql} with params: { {'entityname1': entityname1} }")
#         vessel_data = VesselOut.model_validate(fetch_one(sql, {"entityname1": entityname1}))
        
#     except HTTPException as e:
#         if "æœªæ‰¾åˆ°è¯¥å®ä½“" in str(e.detail):
#             conn = psycopg2.connect(**DB_CONFIG)
#             try:
#                 with conn.cursor() as cursor:
#                     cursor.execute(
#                         "SELECT entityname1 FROM dqs_entity_sanctions_test WHERE entityname1 LIKE %s LIMIT 5",
#                         [{entity_id}]
#                     )
#                     similar_names = [row["entityname1"] for row in cursor.fetchall()]
                    
#                     if similar_names:
#                         raise HTTPException(
#                             status_code=404,
#                             detail={
#                                 "message": f"æœªæ‰¾åˆ°ç²¾ç¡®åŒ¹é… '{entityname1}' çš„è®°å½•",
#                                 "suggestions": similar_names
#                             }
#                         )
#                     else:
#                         raise HTTPException(
#                             status_code=404,
#                             detail=f"æ•°æ®åº“ä¸­æ²¡æœ‰åç§°åŒ…å« '{entityname1}' çš„è®°å½•"
#                         )
#             finally:
#                 conn.close()
#         raise
    
#     # æŸ¥è¯¢è¡¥å……æ•°æ®
#     query_name_data = None
#     try:
#         conn = psycopg2.connect(** DB_CONFIG)
#         with conn.cursor() as cursor:
#             query_sql = """
#             SELECT 
#                 country_nm1, country_nm2, DATEVALUE1, sanctions_lev,
#                 sanctions_list, mid_sanctions_list, no_sanctions_list,
#                 unknown_risk_list, other_list
#             FROM lng.sanctions_risk_result
#             WHERE ENTITYNAME1 = %s
#             LIMIT 1
#             """
#             cursor.execute(query_sql, [entityname1])
#             query_name_data = cursor.fetchone()
#     except Exception as e:
#         logger.error(f"æŸ¥è¯¢åˆ¶è£é£é™©ç»“æœå¤±è´¥: {str(e)}")
#     finally:
#         if conn:
#             conn.close()
    
#     # æ³¨æ„ï¼šå†å²è®°å½•å­˜å‚¨åŠŸèƒ½å·²è¿ç§»åˆ°query_by_nameæ¥å£
#     # å¦‚éœ€è®°å½•æŸ¥è¯¢å†å²ï¼Œè¯·ä½¿ç”¨query_by_nameæ¥å£å¹¶ä¼ å…¥ç”¨æˆ·ä¿¡æ¯å‚æ•°
    
#     return [vessel_data]

# @main_app.get("/search-vessel/", response_model=List[VesselRiskResponse])
# async def search_vessel(
#         vessel_imo: str = Query(..., regex=r"^\d{7}$", example="9263215"),
#         db: Session = Depends(get_db)
# ):
#     """æ ¹æ®IMOç¼–å·æŸ¥è¯¢èˆ¹èˆ¶é£é™©ä¿¡æ¯"""
#     try:
#         query = text("""
#             SELECT 
#                 t0.vessel_imo,
#                 t0.vessel_name,
#                 CASE 
#                     WHEN 'é«˜é£é™©' IN (
#                         t.has_sanctioned_cargo_risk,
#                         t1.has_sanctioned_trades_risk,
#                         t2.has_sanctioned_flag_risk,
#                         t6.has_port_calls_risk
#                     ) THEN 'é«˜é£é™©'
#                     WHEN 'ä¸­é£é™©' IN (
#                         t3.has_ais_gap_risk,
#                         t4.has_sts_events_risk,
#                         t5.has_dark_sts_risk,
#                         t7.has_ais_spoofs_risk
#                     ) THEN 'ä¸­é£é™©'
#                     ELSE 'æ— é£é™©' 
#                 END AS risk_level,
#                 '' AS flag_country,
#                 '' AS risk_type
#             FROM 
#                 (SELECT task_uuid, vessel_imo, vessel_name FROM ods_zyhy_rpa_vessel_risk_data) t0
#             LEFT JOIN 
#                 (SELECT task_uuid, data_type, content_data,
#                     CASE WHEN data_format='Table' THEN 'é«˜é£é™©'
#                          WHEN data_format='Text' AND content_data LIKE 'icon-unverified' THEN 'é«˜é£é™©'
#                          ELSE 'æ— é£é™©' END AS has_sanctioned_cargo_risk
#                  FROM ods_zyhy_rpa_vessel_risk_content WHERE data_type='Cargo') t 
#                  ON t0.task_uuid = t.task_uuid
#             LEFT JOIN 
#                 (SELECT task_uuid, data_type, content_data,
#                     CASE WHEN data_format='Table' AND content_data LIKE 'icon-unverified' THEN 'é«˜é£é™©'
#                          ELSE 'æ— é£é™©' END AS has_sanctioned_trades_risk
#                  FROM ods_zyhy_rpa_vessel_risk_content WHERE data_type='Trade') t1
#                  ON t0.task_uuid = t1.task_uuid
#             LEFT JOIN 
#                 (SELECT task_uuid, data_type, content_data,
#                     CASE WHEN data_format='Table' AND content_data LIKE 'icon-unverified' THEN 'é«˜é£é™©'
#                          ELSE 'æ— é£é™©' END AS has_sanctioned_flag_risk
#                  FROM ods_zyhy_rpa_vessel_risk_content WHERE data_type='Flag') t2
#                  ON t0.task_uuid = t2.task_uuid
#             LEFT JOIN 
#                 (SELECT task_uuid, data_type, content_data,
#                     CASE WHEN data_format='Table' AND content_data LIKE 'icon-unverified' THEN 'ä¸­é£é™©'
#                          ELSE 'æ— é£é™©' END AS has_ais_gap_risk
#                  FROM ods_zyhy_rpa_vessel_risk_content WHERE data_type='AIS gaps') t3
#                  ON t0.task_uuid = t3.task_uuid
#             LEFT JOIN 
#                 (SELECT task_uuid, data_type, content_data,
#                     CASE WHEN data_format='Table' AND content_data LIKE 'icon-unverified' THEN 'ä¸­é£é™©'
#                          ELSE 'æ— é£é™©' END AS has_sts_events_risk
#                  FROM ods_zyhy_rpa_vessel_risk_content WHERE data_type='High risk STS transfers') t4
#                  ON t0.task_uuid = t4.task_uuid
#             LEFT JOIN 
#                 (SELECT task_uuid, data_type, content_data,
#                     CASE WHEN data_format='Table' AND content_data LIKE 'icon-unverified' THEN 'ä¸­é£é™©'
#                          ELSE 'æ— é£é™©' END AS has_dark_sts_risk
#                  FROM ods_zyhy_rpa_vessel_risk_content WHERE data_type='Dark STS transfers') t5
#                  ON t0.task_uuid = t5.task_uuid
#             LEFT JOIN 
#                 (SELECT task_uuid, data_type, content_data,
#                     CASE WHEN data_format='Table' AND content_data LIKE 'icon-unverified' THEN 'é«˜é£é™©'
#                          ELSE 'æ— é£é™©' END AS has_port_calls_risk
#                  FROM ods_byte_zyhy_rpa_vessel_risk_content WHERE data_type='Port call risks') t6
#                  ON t0.task_uuid = t6.task_uuid
#             LEFT JOIN 
#                 (SELECT task_uuid, data_type, content_data,
#                     CASE WHEN data_format='Table' AND content_data LIKE 'icon-unverified' THEN 'ä¸­é£é™©'
#                          ELSE 'æ— é£é™©' END AS has_ais_spoofs_risk
#                  FROM ods_zyhy_rpa_vessel_risk_content WHERE data_type='AIS spoofing') t7
#                  ON t0.task_uuid = t7.task_uuid
#             WHERE 
#                 t0.vessel_imo = :imo
#         """)

#         result = db.execute(query, {"imo": vessel_imo})

#         if not result.rowcount:
#             raise HTTPException(
#                 status_code=404,
#                 detail=f"æœªæ‰¾åˆ°IMOç¼–å·ä¸º {vessel_imo} çš„èˆ¹èˆ¶è®°å½•"
#             )

#         return [
#             VesselRiskResponse(
#                 vessel_imo=row.vessel_imo,
#                 vessel_name=row.vessel_name,
#                 risk_level=row.risk_level,
#                 flag_country=row.flag_country,
#                 risk_type=row.risk_type
#             )
#             for row in result
#         ]

#     except SQLAlchemyError as e:
#         logger.error(f"Database error: {str(e)}")
#         raise HTTPException(status_code=500, detail="æ•°æ®åº“æ“ä½œå¤±è´¥ï¼Œè¯·æ£€æŸ¥æŸ¥è¯¢å‚æ•°")
#     except HTTPException:
#         raise
#     except Exception as e:
#         logger.critical(f"Unexpected error: {str(e)}")
#         raise HTTPException(status_code=500, detail="æœåŠ¡å™¨å†…éƒ¨é”™è¯¯")

# @main_app.post("/api/save_vessel_data")
# async def save_vessel_data(request: Request):
#     """èˆ¹èˆ¶é£é™©æ•°æ®ä¿å­˜"""
#     try:
#         raw_data = await request.json()
#         logger.info(f"æ¥æ”¶åŸå§‹æ•°æ®: {json.dumps(raw_data, indent=2)}")

#         required_fields = ['taskUuid', 'taskStatus', 'vesselImo']
#         for field in required_fields:
#             if field not in raw_data:
#                 raise HTTPException(400, f"ç¼ºå°‘å¿…å¡«å­—æ®µ: {field}")

#         main_data = {
#             "task_uuid": raw_data.get("taskUuid"),
#             "task_status": raw_data.get("taskStatus"),
#             "task_message": raw_data.get("taskMessage"),
#             "task_begin_time": raw_data.get("taskBeginTime", ""),
#             "task_end_time": raw_data.get("taskEndTime", ""),
#             "vessel_imo": str(raw_data.get("vesselImo", "")),
#             "vessel_name": raw_data.get("vesselName", ""),
#             "risk_type": raw_data.get("riskType", ""),
#             "content_json": json.dumps(raw_data.get("content", [])),
#             "raw_content": json.dumps(raw_data.get("rawContent")) if raw_data.get("rawContent") else None
#         }

#         content_items = []
#         for content in raw_data.get("content", []):
#             for data_item in content.get("data", []):
#                 content_items.append({
#                     "task_uuid": raw_data["taskUuid"],
#                     "content_type": content.get("type", "unknown"),
#                     "data_type": data_item.get("type", "unknown"),
#                     "data_format": data_item.get("dataType", data_item.get("data_type", "text")),
#                     "content_data": json.dumps(data_item.get("content", {}))
#                 })

#         conn = psycopg2.connect(
#             host="10.13.16.186",
#             user="coscohw",
#             password="WS8k*123",
#             database="hwda",
#             charset="utf8mb4"
#         )
        
#         try:
#             with conn.cursor() as cursor:
#                 columns = ", ".join(main_data.keys())
#                 placeholders = ", ".join(["%s"] * len(main_data))
#                 sql = f"INSERT INTO ods_zyhy_rpa_vessel_risk_data ({columns}) VALUES ({placeholders})"
#                 cursor.execute(sql, list(main_data.values()))
                
#                 for item in content_items:
#                     columns = ", ".join(item.keys())
#                     placeholders = ", ".join(["%s"] * len(item))
#                     sql = f"INSERT INTO ods_zyhy_rpa_vessel_risk_content ({columns}) VALUES ({placeholders})"
#                     cursor.execute(sql, list(item.values()))
                    
#             conn.commit()
#             logger.info("æ•°æ®ä¿å­˜æˆåŠŸ")
#         except Exception as e:
#             conn.rollback()
#             logger.error(f"æ•°æ®åº“æ“ä½œå¤±è´¥: {str(e)}")
#             raise
#         finally:
#             conn.close()

#         return {
#             "status": "success",
#             "message": "æ•°æ®ä¿å­˜æˆåŠŸ",
#             "task_uuid": raw_data["taskUuid"]
#         }

#     except json.JSONDecodeError:
#         raise HTTPException(400, "æ— æ•ˆçš„JSONæ ¼å¼")
#     except psycopg2.Error as e:
#         logger.error(f"æ•°æ®åº“é”™è¯¯: {str(e)}")
#         raise HTTPException(500, "æ•°æ®åº“æ“ä½œå¤±è´¥")
#     except Exception as e:
#         logger.error(f"æœªå¤„ç†å¼‚å¸¸: {str(e)}", exc_info=True)
#         raise HTTPException(500, "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯")

@main_app.get("/endpoints")
async def list_all_endpoints():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„APIç«¯ç‚¹"""
    return {
        "total_endpoints": 16,
        "endpoints": {
            "maritime_api": {
                "base_path": "/maritime",
                "endpoints": [
                    {
                        "path": "/maritime/",
                        "method": "GET",
                        "description": "æµ·äº‹APIæ ¹è·¯å¾„"
                    },
                    {
                        "path": "/maritime/health",
                        "method": "GET", 
                        "description": "æµ·äº‹APIå¥åº·æ£€æŸ¥"
                    },
                    {
                        "path": "/maritime/api/execute_full_analysis",
                        "method": "POST",
                        "description": "æ‰§è¡Œå®Œæ•´åˆ†ææµç¨‹",
                        "params": ["vessel_imo", "start_date", "end_date"]
                    },
                    {
                        "path": "/maritime/api/lloyds_compliance",
                        "method": "GET",
                        "description": "è·å–Lloyd'såˆè§„æ•°æ®",
                        "params": ["vessel_imo", "start_date", "end_date"]
                    },
                    {
                        "path": "/maritime/api/lloyds_sanctions",
                        "method": "GET",
                        "description": "è·å–Lloyd'såˆ¶è£æ•°æ®",
                        "params": ["vessel_imo"]
                    },
                    {
                        "path": "/maritime/api/uani_check",
                        "method": "GET",
                        "description": "æ£€æŸ¥èˆ¹èˆ¶æ˜¯å¦åœ¨UANIæ¸…å•ä¸­",
                        "params": ["vessel_imo"]
                    },
                    {
                        "path": "/maritime/api/kpler_analysis",
                        "method": "POST",
                        "description": "æ‰§è¡ŒKpleræ•°æ®åˆ†æ",
                        "params": ["vessel_imo", "start_date", "end_date"]
                    },
                    {
                        "path": "/maritime/api/voyage_risks",
                        "method": "GET",
                        "description": "è·å–èˆªæ¬¡é£é™©åˆ†æ",
                        "params": ["vessel_imo", "start_date", "end_date"]
                    },
                    {
                        "path": "/maritime/api/vessel_status",
                        "method": "GET",
                        "description": "è·å–èˆ¹èˆ¶ç»¼åˆçŠ¶æ€",
                        "params": ["vessel_imo"]
                    },
                    {
                        "path": "/maritime/api/save_results",
                        "method": "POST",
                        "description": "ä¿å­˜æ‰€æœ‰åˆ†æç»“æœ",
                        "params": ["output_dir"]
                    },
                    {
                        "path": "/maritime/api/available_endpoints",
                        "method": "GET",
                        "description": "è·å–å¯ç”¨çš„APIç«¯ç‚¹"
                    }
                ]
            },
            "main_app": {
                "base_path": "/",
                "endpoints": [
                    {
                        "path": "/",
                        "method": "GET",
                        "description": "ä¸»åº”ç”¨æ ¹è·¯å¾„"
                    },
                    {
                        "path": "/health",
                        "method": "GET",
                        "description": "ä¸»åº”ç”¨å¥åº·æ£€æŸ¥"
                    },
                    {
                        "path": "/info",
                        "method": "GET",
                        "description": "è·å–æœåŠ¡è¯¦ç»†ä¿¡æ¯"
                    },
                    {
                        "path": "/query_by_name",
                        "method": "GET",
                        "description": "æ ¹æ®ä¼ä¸šIDæŸ¥è¯¢åˆ¶è£é£é™©ç»“æœï¼ˆæ”¯æŒåˆ†é¡µå’Œå†å²è®°å½•ï¼‰",
                        "params": ["entity_id", "page", "page_size", "search_time", "user_id", "user_name", "depart_id", "depart_name"]
                    },
                    {
                        "path": "/search-entities/",
                        "method": "GET",
                        "description": "å…¬å¸é£é™©æŸ¥è¯¢ï¼ˆåŸºç¡€æŸ¥è¯¢åŠŸèƒ½ï¼Œå†å²è®°å½•åŠŸèƒ½å·²è¿ç§»åˆ°query_by_nameï¼‰",
                        "params": ["entityname1", "search_time", "user_id", "user_name", "depart_id", "depart_name"]
                    },
                    {
                        "path": "/search-vessel/",
                        "method": "GET",
                        "description": "æ ¹æ®IMOç¼–å·æŸ¥è¯¢èˆ¹èˆ¶é£é™©ä¿¡æ¯",
                        "params": ["vessel_imo"]
                    },
                    {
                        "path": "/api/save_vessel_data",
                        "method": "POST",
                        "description": "èˆ¹èˆ¶é£é™©æ•°æ®ä¿å­˜",
                        "body": "JSONæ•°æ®"
                    }
                ]
            },
            "purchase_api": {
                "base_path": "/purchase",
                "endpoints": [
                    {
                        "path": "/purchase/vessel_purchase_risk",
                        "method": "POST",
                        "description": "èˆ¹èˆ¶ä¹°å…¥åˆè§„çŠ¶æ€ç­›æŸ¥",
                        "params": ["è¯·æ±‚ä½“å‚æ•°"]
                    }
                ]
            },
            "second_hand_disposal_api": {
                "base_path": "/second_hand",
                "endpoints": [
                    {
                        "path": "/second_hand/vessel_disposal_risk",
                        "method": "POST",
                        "description": "äºŒæ‰‹èˆ¹å‡ºå”®åˆè§„çŠ¶æ€ç­›æŸ¥",
                        "params": ["è¯·æ±‚ä½“å‚æ•°"]
                    }
                ]
            },
            "charter_in_api": {
                "base_path": "/charter_in",
                "endpoints": [
                    {
                        "path": "/charter_in/vessel_charter_risk",
                        "method": "POST",
                        "description": "èˆ¹èˆ¶ç§Ÿå…¥åˆè§„çŠ¶æ€ç­›æŸ¥",
                        "params": ["è¯·æ±‚ä½“å‚æ•°"]
                    }
                ]
            },
            "second_hand_disassemble_api": {
                "base_path": "/second_hand",
                "endpoints": [
                    {
                        "path": "/second_hand/vessel_disassemble_risk",
                        "method": "POST",
                        "description": "äºŒæ‰‹èˆ¹æ‹†è§£åˆè§„çŠ¶æ€ç­›æŸ¥",
                        "params": ["è¯·æ±‚ä½“å‚æ•°"]
                    }
                ]
            }
        }
    }

if __name__ == "__main__":
    # å¯åŠ¨æœåŠ¡å™¨
    # ç®€åŒ–å¯åŠ¨æ—¥å¿—
    
    uvicorn.run(
        "main_server:main_app",
        host="0.0.0.0",
        port=8000,
        #reload=False,  # å¼€å‘æ¨¡å¼å¯ç”¨çƒ­é‡è½½
        log_level="info",
        # æ·»åŠ å¹¶å‘é…ç½®
        workers=1,  # å•è¿›ç¨‹æ¨¡å¼ï¼Œå¤šè¿›ç¨‹ç”±start_server.pyæ§åˆ¶
        access_log=True,
        # è¿æ¥é…ç½®
        limit_concurrency=1000,  # é™åˆ¶å¹¶å‘è¿æ¥æ•°
        limit_max_requests=100000,  # è®¾ç½®ä¸€ä¸ªå¾ˆå¤§çš„æ•°å€¼ï¼Œè€Œä¸æ˜¯0
        timeout_keep_alive=1200,  # keep-aliveè¶…æ—¶æ—¶é—´ï¼ˆç¿»å€ï¼‰
    )